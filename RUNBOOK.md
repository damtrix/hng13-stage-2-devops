# Runbook — Alerting and Operator Actions

This runbook describes the alerts generated by the `alert_watcher` service and recommended operator responses.

Failover detected (Blue → Green or Green → Blue)

- What it means: The watcher detected a change in the upstream `X-App-Pool` header coming from the service responses. This usually means traffic has switched to the other pool.
- Slack alert example: ":rotating_light: Failover detected: BLUE -> GREEN"
- Operator steps:
  1. Check the Nginx config and the `ACTIVE_POOL` environment in `.env` to confirm the intended active pool.
  2. Check the health of the primary container (for the new active pool):
     - docker-compose ps / docker logs for `app-blue` or `app-green`
     - use the health endpoint: curl http://localhost:8081/healthz (blue) or http://localhost:8082/healthz (green)
  3. If the flip was unplanned, inspect application logs for errors. If needed, roll back to the prior release or toggle `ACTIVE_POOL` back using your deployment process.

High 5xx error rate

- What it means: The watcher detected that more than `ERROR_RATE_THRESHOLD` percent of the last `WINDOW_SIZE` requests returned 5xx errors.
- Slack alert example: ":warning: Elevated 5xx error rate: 3.75% over last 200 requests"
- Operator steps:
  1. Verify whether this is a transient spike or sustained. Check the last-minute error logs from the upstream service(s).
  2. Identify whether a particular upstream (app-blue or app-green) is returning 5xx by inspecting container logs and `upstream_addr` shown in the alert.
  3. If one pool is unhealthy, consider failing traffic to the healthy pool (follow your deployment toggling process), or scale/restart the unhealthy service.
  4. If this was caused by a deployment, consider rolling back the release (`RELEASE_ID_*`).

Recovery notice

- What it means: Error rate dropped back below threshold.
- Operator steps:
  1. Confirm service health endpoints are green.
  2. Resume normal monitoring.

Suppressing alerts for planned maintenance

- To temporarily suppress alerts during planned toggles or maintenance windows, set `MAINTENANCE_MODE=true` in the `.env` file and restart the `alert_watcher` service. This will cause the watcher to suppress failover and error-rate alerts.

Key files and locations

- Nginx access log (shared volume): `/var/log/nginx/custom_access.log` (mounted into host via docker-compose volume `nginx_logs`)
- Watcher code: `./alert_watcher/watcher.py`
- Environment template: `env.template` -> copy to `.env` and populate `SLACK_WEBHOOK_URL`.

Notes

- Alerts are throttled by `ALERT_COOLDOWN_SEC` to avoid alert spam.
- The watcher relies only on access logs and does not inspect request paths or application internals.
